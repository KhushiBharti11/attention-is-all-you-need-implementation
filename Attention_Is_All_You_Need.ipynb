{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer from Scratch\n",
        "### Based on \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
        "\n",
        "This notebook implements the core components of the Transformer model,\n",
        "including scaled dot-product attention, multi-head attention, positional\n",
        "encoding, and encoder–decoder blocks, with explanations mapping directly\n",
        "to the original research paper.\n"
      ],
      "metadata": {
        "id": "6dCKzyXB5KVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapping to Research Paper\n",
        "\n",
        "This implementation directly follows Section 3 of the paper\n",
        "\"Attention Is All You Need\" (Vaswani et al., 2017):\n",
        "\n",
        "- Section 3.1: Encoder–Decoder architecture\n",
        "- Section 3.2.1: Scaled dot-product attention\n",
        "- Section 3.2.2: Multi-head attention\n",
        "- Section 3.2.3: Masked self-attention in decoder\n",
        "- Section 3.5: Positional encoding\n"
      ],
      "metadata": {
        "id": "FPDU1sfG91gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n"
      ],
      "metadata": {
        "id": "uotfUfmB5QL3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration (small on purpose for clarity)\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "d_k = d_model // num_heads\n"
      ],
      "metadata": {
        "id": "qjUO4qjj6Dth"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Scaled Dot-Product Attention\n",
        "\n",
        "Scaled dot-product attention computes the similarity between queries and keys\n",
        "using a dot product, scales the result by √d_k to stabilize gradients, applies\n",
        "softmax to obtain attention weights, and uses these weights to compute a\n",
        "weighted sum of values.\n"
      ],
      "metadata": {
        "id": "Y4Pi3SKt6GRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k):\n",
        "        super().__init__()\n",
        "        self.scale = math.sqrt(d_k)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Q, K, V: (batch_size, seq_len, d_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "        # scores: (batch_size, seq_len, seq_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return output, attention_weights\n"
      ],
      "metadata": {
        "id": "7ou5Lyw16KPz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B, S, d_k = 2, 5, 16\n",
        "Q = torch.rand(B, S, d_k)\n",
        "K = torch.rand(B, S, d_k)\n",
        "V = torch.rand(B, S, d_k)\n",
        "\n",
        "attention = ScaledDotProductAttention(d_k)\n",
        "output, attn_weights = attention(Q, K, V)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Attention weights shape:\", attn_weights.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC_VDoVG6MHB",
        "outputId": "b8436ef8-2876-4ec2-80b8-5d32007b2e9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 5, 16])\n",
            "Attention weights shape: torch.Size([2, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Multi-Head Attention\n",
        "\n",
        "Multi-head attention allows the model to jointly attend to information from\n",
        "different representation subspaces at different positions. Each head performs\n",
        "scaled dot-product attention independently, and the results are concatenated\n",
        "and linearly transformed.\n"
      ],
      "metadata": {
        "id": "_HEJ1NYP6Nzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(self.d_k)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        B, S, _ = Q.size()\n",
        "\n",
        "        Q = self.W_q(Q)\n",
        "        K = self.W_k(K)\n",
        "        V = self.W_v(V)\n",
        "\n",
        "        Q = Q.view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        out, _ = self.attention(Q, K, V, mask)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        out = out.view(B, S, self.num_heads * self.d_k)\n",
        "\n",
        "        return self.W_o(out)\n"
      ],
      "metadata": {
        "id": "ltV25-8t6c87"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B, S, d_model = 2, 6, 128\n",
        "x = torch.rand(B, S, d_model)\n",
        "\n",
        "mha = MultiHeadAttention(d_model, num_heads=8)\n",
        "out = mha(x, x, x)\n",
        "\n",
        "print(\"Output shape:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd635WG46fMQ",
        "outputId": "7fd672c8-77af-4642-d0d9-7e69b71e2f2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 6, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Positional Encoding\n",
        "\n",
        "Since the Transformer does not use recurrence or convolution, positional\n",
        "encoding is added to the input embeddings to provide information about\n",
        "the relative or absolute position of tokens in the sequence. The original\n",
        "paper uses fixed sinusoidal functions.\n"
      ],
      "metadata": {
        "id": "xg62WluE6hNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "GVoxLlxz6pzc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B, S, d_model = 2, 10, 128\n",
        "x = torch.zeros(B, S, d_model)\n",
        "\n",
        "pe = PositionalEncoding(d_model)\n",
        "out = pe(x)\n",
        "\n",
        "print(\"Output shape:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3GPYr8E6rm0",
        "outputId": "735163a2-d3ba-4ebe-8d97-e998d63904c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 10, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Encoder Block\n",
        "\n",
        "Each encoder layer consists of a multi-head self-attention mechanism\n",
        "followed by a position-wise feed-forward network. Residual connections\n",
        "and layer normalization are applied after each sub-layer.\n"
      ],
      "metadata": {
        "id": "qScvPfee6xqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "9s3st5Th6-kB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_out = self.mha(x, x, x, mask)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "MiNlwrfl7AZ5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B, S, d_model = 2, 8, 128\n",
        "x = torch.rand(B, S, d_model)\n",
        "\n",
        "encoder_block = EncoderBlock(d_model, num_heads=8)\n",
        "out = encoder_block(x)\n",
        "\n",
        "print(\"Encoder output shape:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrSUwmHC7CcA",
        "outputId": "f0dd9c31-554a-4dac-f4a7-b96618675225"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: torch.Size([2, 8, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Decoder Block\n",
        "\n",
        "Each decoder layer contains masked multi-head self-attention, followed by\n",
        "encoder–decoder attention and a position-wise feed-forward network.\n",
        "Masking prevents the model from attending to future positions during training.\n"
      ],
      "metadata": {
        "id": "49e-82H27EMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_subsequent_mask(size):\n",
        "    mask = torch.tril(torch.ones(size, size))\n",
        "    return mask\n"
      ],
      "metadata": {
        "id": "4QDcYlHx7Waj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
        "        attn1 = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + attn1)\n",
        "\n",
        "        attn2 = self.enc_attn(x, enc_out, enc_out, src_mask)\n",
        "        x = self.norm2(x + attn2)\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm3(x + ffn_out)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZOHHNGLg7YUr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B, S, d_model = 2, 6, 128\n",
        "x = torch.rand(B, S, d_model)\n",
        "enc_out = torch.rand(B, S, d_model)\n",
        "\n",
        "tgt_mask = generate_subsequent_mask(S)\n",
        "\n",
        "decoder = DecoderBlock(d_model, num_heads=8)\n",
        "out = decoder(x, enc_out, tgt_mask=tgt_mask)\n",
        "\n",
        "print(\"Decoder output shape:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QI_-Sxi7avi",
        "outputId": "bfa00e58-aafe-4f44-9ea8-b84a218a8d32"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: torch.Size([2, 6, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Full Transformer Model\n",
        "\n",
        "The full Transformer model consists of stacked encoder and decoder blocks.\n",
        "Token embeddings are combined with positional encodings before being passed\n",
        "through the encoder. The decoder generates outputs using masked self-attention\n",
        "and encoder–decoder attention.\n"
      ],
      "metadata": {
        "id": "-nXOPUZf7c3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_layers=2, vocab_size=1000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "        self.encoders = nn.ModuleList(\n",
        "            [EncoderBlock(d_model, num_heads) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.decoders = nn.ModuleList(\n",
        "            [DecoderBlock(d_model, num_heads) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        src = self.embedding(src)\n",
        "        src = self.positional_encoding(src)\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            src = encoder(src, src_mask)\n",
        "\n",
        "        tgt = self.embedding(tgt)\n",
        "        tgt = self.positional_encoding(tgt)\n",
        "\n",
        "        for decoder in self.decoders:\n",
        "            tgt = decoder(tgt, src, src_mask, tgt_mask)\n",
        "\n",
        "        return self.fc_out(tgt)\n"
      ],
      "metadata": {
        "id": "UTCo1t4r7k9m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B, S = 2, 5\n",
        "vocab_size = 1000\n",
        "\n",
        "src = torch.randint(0, vocab_size, (B, S))\n",
        "tgt = torch.randint(0, vocab_size, (B, S))\n",
        "\n",
        "tgt_mask = generate_subsequent_mask(S)\n",
        "\n",
        "model = Transformer(d_model=128, num_heads=8, num_layers=2, vocab_size=vocab_size)\n",
        "out = model(src, tgt, tgt_mask=tgt_mask)\n",
        "\n",
        "print(\"Transformer output shape:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKZzd6uv7nIV",
        "outputId": "4dc9e5b6-8931-407e-b9c9-17bf79e4cfbe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer output shape: torch.Size([2, 5, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Shift target for teacher forcing\n",
        "loss = criterion(out.view(-1, vocab_size), tgt.view(-1))\n",
        "print(\"Sample loss:\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUUAuOmG7pSy",
        "outputId": "e9fcdf28-7703-43a5-c766-9b5b2f9a2b55"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample loss: 7.0142412185668945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal training demonstration (sanity check)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    src = torch.randint(0, vocab_size, (B, S))\n",
        "    tgt = torch.randint(0, vocab_size, (B, S))\n",
        "    tgt_mask = generate_subsequent_mask(S)\n",
        "\n",
        "    output = model(src, tgt, tgt_mask=tgt_mask)\n",
        "    loss = criterion(output.view(-1, vocab_size), tgt.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlwG0wCE7sgm",
        "outputId": "b3acbc07-a9f4-415b-87f5-92f7b200b147"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.1733\n",
            "Epoch 2, Loss: 6.9667\n",
            "Epoch 3, Loss: 7.1999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H1PIXPs-8Z7_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}